# 机器学习中的线性模型详解

## 目录
- [一、基本形式](#一基本形式)
- [二、线性回归（Linear Regression）](#二线性回归linear-regression)
  - [1. 模型与目标](#1-模型与目标)
  - [2. 模型假设](#2-模型假设)
  - [3. 参数求解](#3-参数求解)
    - [3.1 最小二乘法（OLS）](#31-最小二乘法ols)
    - [3.2 梯度下降法](#32-梯度下降法)
  - [4. 模型评估](#4-模型评估)
- [三、逻辑回归（Logistic Regression）](#三逻辑回归logistic-regression)
  - [1. 模型原理](#1-模型原理)
  - [2. 参数估计](#2-参数估计)
  - [3. 多分类扩展](#3-多分类扩展)
    - [3.1 One-vs-Rest (OvR)](#31-one-vs-rest-ovr)
    - [3.2 Softmax回归](#32-softmax回归)
- [四、正则化（Regularization）](#四正则化regularization)
  - [1. 原理](#1-原理)
  - [2. 方法](#2-方法)
- [五、总结](#五总结)

---

## 一、基本形式
线性模型通过特征的线性组合进行预测：
$$
y = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
$$
- **向量形式**：  
  $$
  y = \mathbf{w}^T \mathbf{x} + b
  $$  
  其中 $\mathbf{w}$ 为权重向量，$b$ 为偏置项。
- **特点**：模型参数直观反映特征重要性。

---

## 二、线性回归（Linear Regression）
### 1. 模型与目标
- **预测函数**：  
  $$
  y = \mathbf{w}^T \mathbf{x} + b
  $$
- **目标**：最小化均方误差（MSE）：  
  $$
  L(\mathbf{w}, b) = \frac{1}{m} \sum_{i=1}^m \left( y_i - \mathbf{w}^T \mathbf{x}_i - b \right)^2
  $$

### 2. 模型假设
- **统计假设**：
  1. **线性关系**：目标变量与特征呈线性关系。
  2. **误差独立同分布**：误差项 $\epsilon \sim \mathcal{N}(0, \sigma^2)$，且相互独立。
  3. **无多重共线性**：特征间不高度相关。
  4. **同方差性**：误差的方差在特征空间内恒定。

### 3. 参数求解
#### 3.1 最小二乘法（OLS）
- **解析解**：  
  $$
  \mathbf{w}^* = (X^T X)^{-1} X^T \mathbf{y}
  $$
- **条件**：$X^T X$ 需可逆（若不可逆，需使用正则化或降维）。

#### 3.2 梯度下降法
- **梯度计算**：  
  $$
  \nabla L(\mathbf{w}) = \frac{2}{m} X^T (X\mathbf{w} - \mathbf{y})
  $$
- **参数更新**：  
  $$
  \mathbf{w} := \mathbf{w} - \eta \nabla L(\mathbf{w})
  $$

### 4. 模型评估
- **常用指标**：
  1. **均方误差（MSE）**：  
     $$
     \text{MSE} = \frac{1}{m} \sum_{i=1}^m (y_i - \hat{y}_i)^2
     $$
  2. **均方根误差（RMSE）**：  
     $$
     \text{RMSE} = \sqrt{\text{MSE}}
     $$
  3. **$R^2$ 分数**：  
     $$
     R^2 = 1 - \frac{\sum_{i=1}^m (y_i - \hat{y}_i)^2}{\sum_{i=1}^m (y_i - \bar{y})^2}
     $$

---

## 三、逻辑回归（Logistic Regression）
### 1. 模型原理
- **sigmoid 函数**：  
  $$
  \sigma(z) = \frac{1}{1 + e^{-z}}
  $$
- **预测概率**：  
  $$
  P(y=1 \mid \mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x} + b)
  $$
- **决策边界**：$\mathbf{w}^T \mathbf{x} = 0$ 为分类超平面。

### 2. 参数估计
- **交叉熵损失函数**：  
  $$
  J(\mathbf{w}) = -\frac{1}{m} \sum_{i=1}^m \left[ y_i \log \sigma(\mathbf{w}^T \mathbf{x}_i) + (1 - y_i) \log \left( 1 - \sigma(\mathbf{w}^T \mathbf{x}_i) \right) \right]
  $$
- **梯度计算**：  
  $$
  \nabla J(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^m \left( \sigma(\mathbf{w}^T \mathbf{x}_i) - y_i \right) \mathbf{x}_i
  $$
- **参数更新**：  
  $$
  \mathbf{w} := \mathbf{w} - \eta \nabla J(\mathbf{w})
  $$

### 3. 多分类扩展
#### 3.1 One-vs-Rest (OvR)
- 训练 $K$ 个二分类器，第 $k$ 个分类器判断样本是否属于第 $k$ 类。
- 预测时选择概率最高的类别。

#### 3.2 Softmax 回归
- **概率分布**：  
  $$
  P(y=k \mid \mathbf{x}) = \frac{e^{\mathbf{w}_k^T \mathbf{x}}}{\sum_{j=1}^K e^{\mathbf{w}_j^T \mathbf{x}}}
  $$
- **多类交叉熵损失**：  
  $$
  J(\mathbf{W}) = -\frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \mathbf{1}\{y_i = k\} \log P(y=k \mid \mathbf{x}_i)
  $$

---

## 四、正则化（Regularization）
### 1. 原理
- **正则化目标函数**：  
  $$
  L_{\text{正则化}} = L_{\text{原始损失}} + \lambda \cdot R(\mathbf{w})
  $$

### 2. 方法
- **L1 正则化（Lasso）**：  
  $$
  R(\mathbf{w}) = \|\mathbf{w}\|_1 \quad \text{（稀疏性，特征选择）}
  $$
- **L2 正则化（Ridge）**：  
  $$
  R(\mathbf{w}) = \|\mathbf{w}\|_2^2 \quad \text{（平滑权重，防止过拟合）}
  $$
- **Elastic Net**：  
  $$
  R(\mathbf{w}) = \alpha \|\mathbf{w}\|_1 + (1-\alpha)\|\mathbf{w}\|_2^2
  $$

---

## 五、总结
- **线性回归**：通过最小化均方误差拟合连续值，需满足线性假设和误差独立同分布。
- **逻辑回归**：利用 sigmoid 函数建模概率，通过交叉熵损失进行参数估计，支持二分类与多分类。
- **正则化**：L1 正则化促进稀疏性，L2 正则化控制权重幅度，Elastic Net 结合两者优势。
- **扩展性**：通过 Softmax 函数和正则化技术可适应复杂分类场景。