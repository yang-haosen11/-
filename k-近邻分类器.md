# 最近邻分类器（k-NN）超参数原理详解

## 目录
- [1. k值（邻居数量）](#1-k值邻居数量)
- [2. 距离度量](#2-距离度量)
- [3. 权重函数](#3-权重函数)
- [4. 搜索算法参数](#4-搜索算法参数)
- [5. 其他超参数](#5-其他超参数)
- [6. 超参数间的相互影响](#6-超参数间的相互影响)
- [7. 如何选择超参数](#7-如何选择超参数)

---

<a name="1-k值邻居数量"></a>
### 1. k值（邻居数量）
- **定义**：预测时参考的最近邻数量。
- **数学原理**：
  - **分类任务**：多数投票决定类别。
  - **回归任务**：k个邻居目标值的均值。
- **影响**：
  - **k=1**：高方差（过拟合），对噪声敏感。
  - **k较大**：高偏差（欠拟合），平滑噪声。
- **选择方法**：
  - 交叉验证寻找最优k值。
  - 经验公式：初始k ≈ √N（N为样本数）。

---

<a name="2-距离度量"></a>
### 2. 距离度量
| 类型                | 公式                               | 适用场景                   |
|---------------------|------------------------------------|---------------------------|
| **欧氏距离**        | $ \sqrt{\sum (x_i - y_i)^2} $   | 连续特征，需标准化        |
| **曼哈顿距离**      | $ \sum \|x_i - y_i\| $          | 高维稀疏数据（如文本）    |
| **余弦相似度**      | $ \frac{x \cdot y}{\|x\| \|y\|} $ | 方向敏感数据（如推荐系统）|
| **明可夫斯基距离**  | $ (\sum \|x_i - y_i\|^p)^{1/p} $ | 泛化形式（p为超参数）     |
| **马氏距离**        | $ \sqrt{(x - y)^T S^{-1} (x - y)} $ | 非独立同分布数据       |

**标准化要求**：欧氏/曼哈顿距离需Z-Score标准化。

---

<a name="3-权重函数"></a>
### 3. 权重函数
- **均匀权重**：所有邻居权重相同。
- **距离倒数加权**：权重与距离成反比，如 $ w_i = \frac{1}{d(x, x_i)} $。

```python
  # sklearn示例：权重与距离成反比
  model = KNeighborsClassifier(weights='distance')
  ```

- **高斯核加权**：
  $$
  w_i = \exp\left(-\frac{d(x, x_i)^2}{2\sigma^2}\right)
  $$
  - σ控制权重衰减速度。

---

<a name="4-搜索算法参数"></a>
### 4. 搜索算法参数
| 算法       | 时间复杂度     | 适用场景               | 关键参数         |
|------------|---------------|-----------------------|------------------|
| **Brute-Force** | O(N)          | 小规模数据            | -                |
| **KD树**    | O(D log N)    | 低维数据（D < 20）     | `leaf_size`      |
| **Ball Tree** | O(D log N)   | 高维数据              | `leaf_size`      |
| **ANN**     | O(1)~O(log N) | 超大规模数据（近似解）| `epsilon`（精度）|

---

<a name="5-其他超参数"></a>
### 5. 其他超参数
- **数据标准化**：对特征进行Z-Score标准化以消除量纲影响。
- **多分类策略**：支持多数投票或概率投票机制。

---

<a name="6-超参数间的相互影响"></a>
### 6. 超参数间的相互影响
| 参数组合             | 场景                     | 优化策略                      |
|----------------------|--------------------------|-------------------------------|
| **高维数据 + 大k值** | 距离失效，分类效果差     | 降维（PCA） + 减小k值         |
| **噪声数据 + 小k值** | 模型不稳定               | 增大k值 + 距离加权            |
| **大规模数据 + KD树**| 内存溢出                | 切换为Ball Tree或近似算法     |

---

<a name="7-如何选择超参数"></a>
### 7. 如何选择超参数

#### **1. k值选择**
- **交叉验证法**：
  - 将数据分为训练集和验证集，尝试不同k值（如1到20），选择验证集精度最高的k。
  - 避免使用测试集调参，防止过拟合。
- **经验法则**：
  - 初始尝试：k ≈ √N（N为训练样本数）。
  - 类别均衡数据：优先奇数k值（避免平票）。
  - 噪声数据：增大k值（如5~15）以平滑异常点影响。

#### **2. 距离度量选择**
- **数据特性驱动**：
  - **连续特征**：默认欧氏距离，需标准化。
  - **高维稀疏数据**（如文本）：余弦相似度或曼哈顿距离。
  - **地理空间数据**：曼哈顿距离（网格路径）或哈弗辛公式（球面距离）。
  - **特征相关性高**：马氏距离（需计算协方差矩阵）。
- **验证对比**：
  - 对候选距离函数（如欧氏 vs 余弦）分别训练模型，比较验证集效果。

#### **3. 权重函数选择**
- **场景建议**：
  - **均匀权重**：数据分布均匀且k值适中时。
  - **距离倒数加权**：希望近邻对预测影响更大时（如局部特征显著的数据）。
  - **高斯加权**：需要平滑衰减权重时，需调参σ（带宽参数）。

#### **4. 搜索算法优化**
- **数据规模与维度**：
  - **小数据（N < 1k）**：暴力搜索（Brute-Force）无需优化。
  - **中低维（D < 20）**：KD树（调整`leaf_size=30~50`）。
  - **高维或非结构化数据**：Ball Tree 或近似最近邻（ANN）。
- **内存限制**：
  - 数据量超过1e5时，优先使用近似算法（如`annoy`或`faiss`库）。

#### **5. 数据预处理**
- **标准化必要性与方法**：
  - **必需场景**：使用欧氏/曼哈顿距离时，用Z-Score标准化（均值=0，方差=1）。
  - **可选场景**：余弦相似度可跳过标准化，直接归一化向量长度。
- **降维处理**：
  - 当维度 > 100时，使用PCA或t-SNE降低维度至20~50，再应用k-NN。

#### **6. 自动化调参工具**
- **网格搜索（GridSearchCV）**：
  - 示例参数网格：
    ```yaml
    params = {
        'n_neighbors': [3, 5, 7, 9],
        'metric': ['euclidean', 'manhattan'],
        'weights': ['uniform', 'distance']
    }
    ```
  - 通过交叉验证穷举所有组合，选择最优参数。
- **贝叶斯优化（如Optuna）**：
  - 适用于超参数空间较大时，以更少迭代找到接近最优解。
